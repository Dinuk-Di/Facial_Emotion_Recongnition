{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1838af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 'angry': 3500 train, 750 val, 750 test images.\n",
      "Class 'boring': 3500 train, 750 val, 750 test images.\n",
      "Class 'disgust': 3500 train, 750 val, 750 test images.\n",
      "Class 'fear': 3500 train, 750 val, 750 test images.\n",
      "Class 'happy': 3500 train, 750 val, 750 test images.\n",
      "Class 'neutral': 3500 train, 750 val, 750 test images.\n",
      "Class 'sad': 3500 train, 750 val, 750 test images.\n",
      "Class 'stress': 3500 train, 750 val, 750 test images.\n",
      "Class 'surprise': 3500 train, 750 val, 750 test images.\n",
      "Dataset split completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "original_dataset_dir = r'D:\\RuhunaNew\\Academic\\Research\\Facial_Recog_Repo\\Group_50_Repo\\Datasets\\Dataset_48_48_5000_per_each'  # Folder containing 9 emotion subfolders\n",
    "output_dir = r'D:\\RuhunaNew\\Academic\\Research\\Facial_Recog_Repo\\Group_50_Repo\\Datasets\\Preprocesssed_5000'  # Where train/val/test folders will be created\n",
    "\n",
    "# Split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Create output directories for train, val, test with class subfolders\n",
    "for split in ['train', 'val', 'test']:\n",
    "    for emotion_class in os.listdir(original_dataset_dir):\n",
    "        class_dir = os.path.join(output_dir, split, emotion_class)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "# For each class folder, split files and copy\n",
    "for emotion_class in os.listdir(original_dataset_dir):\n",
    "    class_path = os.path.join(original_dataset_dir, emotion_class)\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # First split train and temp (val+test)\n",
    "    train_files, temp_files = train_test_split(images, train_size=train_ratio, random_state=42, shuffle=True)\n",
    "    # Then split temp into val and test\n",
    "    val_size_adjusted = val_ratio / (val_ratio + test_ratio)  # Adjust val ratio relative to temp\n",
    "    val_files, test_files = train_test_split(temp_files, train_size=val_size_adjusted, random_state=42, shuffle=True)\n",
    "\n",
    "    # Copy files to respective folders\n",
    "    for split_name, file_list in zip(['train', 'val', 'test'], [train_files, val_files, test_files]):\n",
    "        for file_name in file_list:\n",
    "            src = os.path.join(class_path, file_name)\n",
    "            dst = os.path.join(output_dir, split_name, emotion_class, file_name)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "    print(f\"Class '{emotion_class}': {len(train_files)} train, {len(val_files)} val, {len(test_files)} test images.\")\n",
    "\n",
    "print(\"Dataset split completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# ---------------------------\n",
    "# Configurations and Globals\n",
    "# ---------------------------\n",
    "IMG_SIZE = 128          # CNN input size (variable)\n",
    "NUM_CLASSES = 9        # Number of emotion classes\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20\n",
    "PATIENCE = 5           # Early stopping patience\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------------\n",
    "# Small CNN for Grayscale Emotion Classification\n",
    "# ---------------------------\n",
    "class SmallEmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(SmallEmotionCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # 1 x IMG_SIZE x IMG_SIZE\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 16 x IMG_SIZE/2 x IMG_SIZE/2\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 32 x IMG_SIZE/4 x IMG_SIZE/4\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # 64 x IMG_SIZE/8 x IMG_SIZE/8\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * (IMG_SIZE // 8) * (IMG_SIZE // 8), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Class with YOLOv11 Face Detection\n",
    "# ---------------------------\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, yolo_model, transform=None):\n",
    "        \"\"\"\n",
    "        image_paths: list of grayscale image file paths\n",
    "        labels: list of integer labels (0 to NUM_CLASSES-1)\n",
    "        yolo_model: loaded YOLOv11 model for face detection\n",
    "        transform: torchvision transforms applied to cropped face\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.yolo_model = yolo_model\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load grayscale image with OpenCV\n",
    "        gray_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if gray_img is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "        # Convert grayscale to 3-channel by duplicating channels for YOLO\n",
    "        img_rgb = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        # Run YOLOv11 face detection\n",
    "        results = self.yolo_model(img_rgb)\n",
    "\n",
    "        # Extract bounding boxes with confidence > 0.5\n",
    "        boxes = []\n",
    "        for r in results:\n",
    "            for box in r.boxes:\n",
    "                if box.conf > 0.5:\n",
    "                    boxes.append(box.xyxy.cpu().numpy().astype(int))\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            # If no face detected, use full image\n",
    "            h, w = gray_img.shape\n",
    "            boxes = [np.array([0, 0, w, h])]\n",
    "\n",
    "        # Take the largest box (assuming main face)\n",
    "        boxes = sorted(boxes, key=lambda b: (b[2]-b[0])*(b[3]-b[1]), reverse=True)\n",
    "        x1, y1, x2, y2 = boxes[0]\n",
    "\n",
    "        # Crop face from grayscale image\n",
    "        face = gray_img[y1:y2, x1:x2]\n",
    "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        face_pil = Image.fromarray(face)\n",
    "        if self.transform:\n",
    "            face_tensor = self.transform(face_pil)\n",
    "        else:\n",
    "            face_tensor = transforms.ToTensor()(face_pil)  # This will produce 1 channel tensor\n",
    "\n",
    "        return face_tensor, label\n",
    "\n",
    "# ---------------------------\n",
    "# Data Transforms for Grayscale\n",
    "# ---------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts to 1xHxW tensor and scales [0,255] to [0,1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize grayscale\n",
    "])\n",
    "\n",
    "# ---------------------------\n",
    "# Training and Validation Functions\n",
    "# ---------------------------\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# ---------------------------\n",
    "# Early Stopping Class\n",
    "# ---------------------------\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=PATIENCE, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_model_wts = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_wts = model.state_dict()\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss improved to {val_loss:.4f}, saving model.\")\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"Validation loss did not improve. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "# ---------------------------\n",
    "# Main Training Loop\n",
    "# ---------------------------\n",
    "def main(train_image_paths, train_labels, val_image_paths, val_labels):\n",
    "    # Load YOLOv11 face detection model (YOLO11n lightweight)\n",
    "    yolo_model = YOLO(\"yolo11n.pt\")  # Adjust path to your YOLOv11 weights\n",
    "\n",
    "    # Create datasets and loaders\n",
    "    train_dataset = EmotionDataset(train_image_paths, train_labels, yolo_model, transform=transform)\n",
    "    val_dataset = EmotionDataset(val_image_paths, val_labels, yolo_model, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Initialize CNN model, loss, optimizer\n",
    "    model = SmallEmotionCNN().to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=True)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} - \"\n",
    "              f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(early_stopping.best_model_wts)\n",
    "    print(\"Training complete. Best validation loss: {:.4f}\".format(early_stopping.best_loss))\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"emotion_cnn_yolo_best.pth\")\n",
    "    print(\"Best model saved as emotion_cnn_best.pth\")\n",
    "\n",
    "    return model\n",
    "\n",
    "import os\n",
    "\n",
    "def get_image_paths_and_labels(root_dir):\n",
    "    \"\"\"\n",
    "    Scans root_dir for subfolders (emotions), collects image paths and integer labels.\n",
    "    Returns:\n",
    "        image_paths: list of image file paths\n",
    "        labels: list of integer labels corresponding to each path\n",
    "        class_to_idx: dict mapping class name to label index\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    class_names = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "    class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_names)}\n",
    "    \n",
    "    for cls_name in class_names:\n",
    "        cls_folder = os.path.join(root_dir, cls_name)\n",
    "        for fname in os.listdir(cls_folder):\n",
    "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_paths.append(os.path.join(cls_folder, fname))\n",
    "                labels.append(class_to_idx[cls_name])\n",
    "    return image_paths, labels, class_to_idx\n",
    "\n",
    "# Example usage:\n",
    "train_root = r\"D:\\RuhunaNew\\Academic\\Research\\Facial_Recog_Repo\\Group_50_Repo\\Datasets\\Preprocesssed_5000\\train\"  # e.g., /data/emotions/train\n",
    "test_root = r\"D:\\RuhunaNew\\Academic\\Research\\Facial_Recog_Repo\\Group_50_Repo\\Datasets\\Preprocesssed_5000\\test\"    # e.g., /data/emotions/test\n",
    "\n",
    "train_image_paths, train_labels, class_to_idx = get_image_paths_and_labels(train_root)\n",
    "test_image_paths, test_labels, _ = get_image_paths_and_labels(test_root)\n",
    "\n",
    "# Now call main() with these lists:\n",
    "trained_model = main(train_image_paths, train_labels, test_image_paths, test_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
