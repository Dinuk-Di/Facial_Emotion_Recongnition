{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129cd92b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\loq\\anaconda3\\envs\\crewai-env\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\loq\\anaconda3\\envs\\crewai-env\\lib\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\loq\\anaconda3\\envs\\crewai-env\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\loq\\anaconda3\\envs\\crewai-env\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\loq\\anaconda3\\envs\\crewai-env\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\loq\\anaconda3\\envs\\crewai-env\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Connection timed out while downloading.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\LOQ\\\\AppData\\\\Local\\\\Temp\\\\pip-unpack-p2foji14\\\\fsspec-2025.3.0-py3-none-any.whl.metadata'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers accelerate torch scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4104c687",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[32m      4\u001b[39m dataset = load_dataset(\u001b[33m'\u001b[39m\u001b[33mjbarat/plant_species\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('jbarat/plant_species')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c9f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['aechmea_fasciata', \n",
    "'agave_americana', \n",
    "'agave_attenuata', \n",
    "'agave_tequilana', \n",
    "'aglaonema_commutatum', \n",
    "'albuca_spiralis',\n",
    " 'allium_cepa', \n",
    "'allium_sativum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118fdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into train and test (let's say 10% for the test set)\n",
    "train_test_split = dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Further split the training set to get a validation set (e.g., 10% of the training set)\n",
    "train_val_split = train_test_split['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Combine the splits into a new DatasetDict\n",
    "final_dataset = {\n",
    "    'train': train_val_split['train'],\n",
    "    'val': train_val_split['test'],  \n",
    "    'test': train_test_split['test']  \n",
    "}\n",
    "\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf179331",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = final_dataset[\"train\"]\n",
    "val_ds = final_dataset[\"val\"]\n",
    "test_ds = final_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae84918",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_ds[1]['image']\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shown_labels = set()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loop through the dataset and plot the first image of each label\n",
    "for i, sample in enumerate(train_ds):\n",
    "    label = train_ds.features[\"label\"].names[sample[\"label\"]]\n",
    "    if label not in shown_labels:\n",
    "        plt.subplot(1, len(train_ds.features[\"label\"].names), len(shown_labels) + 1)\n",
    "        plt.imshow(sample[\"image\"])\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "        shown_labels.add(label)\n",
    "        if len(shown_labels) == len(train_ds.features[\"label\"].names):\n",
    "            break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id: label for id, label in enumerate(train_ds.features[\"label\"].names)}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "id2label, id2label[train_ds[0][\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf63020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "\n",
    "model_name = \"google/vit-large-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    ToTensor,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "# Get configurations from ViT processor\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "# Normalizes the image pixels by subtracting the mean and dividing by the std from the pretrained model configurations\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "# Compose: Combines a series of image transformations into one pipeline.\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "test_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb89f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_train_transforms(examples):\n",
    "    examples[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def apply_val_transforms(examples):\n",
    "    examples[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "def apply_test_transforms(examples):\n",
    "    examples[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
    "    return examples\n",
    "\n",
    "train_ds.set_transform(apply_train_transforms)\n",
    "val_ds.set_transform(apply_val_transforms)\n",
    "test_ds.set_transform(apply_test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Stacks the pixel values of all examples into a single tensor and collects labels into a tensor\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# Create a DataLoader for the training dataset, with custom collation and a batch size of 4\n",
    "train_dl = DataLoader(train_ds, collate_fn=collate_fn, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)\n",
    "\n",
    "# Output\n",
    "# pixel_values torch.Size([4, 3, 224, 224])\n",
    "# labels torch.Size([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels  = dataset['train'].features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels = len(labels),\n",
    "    id2label=id2label, \n",
    "    label2id=label2id, \n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"output-models\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=2,\n",
    "  fp16=True,\n",
    "  save_steps=10,\n",
    "  eval_steps=10,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  report_to='tensorboard',\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    tokenizer=processor,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(test_ds)\n",
    "print(outputs.metrics)\n",
    "\n",
    "# Output\n",
    "# {'test_loss': 0.25027137994766235, \n",
    "# 'test_runtime': 1.3596, \n",
    "# 'test_samples_per_second': 58.842, \n",
    "# 'test_steps_per_second': 7.355}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = id2label.values()\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d16c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = train_ds.features[\"label\"].names\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# Load model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('vit_jit_emotion_model.pt').to(device)\n",
    "model.eval()\n",
    "\n",
    "# Class labels (update with your emotions)\n",
    "class_names = ['angry', 'boring', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'stress', 'suprised']\n",
    "\n",
    "# Preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def predict_emotion(image_path):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        confidence, class_idx = torch.max(probabilities, 0)\n",
    "    \n",
    "    return class_names[class_idx.item()], confidence.item()\n",
    "\n",
    "# Example usage\n",
    "emotion, confidence = predict_emotion('test_image.jpg')\n",
    "print(f'Predicted Emotion: {emotion} | Confidence: {confidence:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load transformer model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.jit.load('D:/RuhunaNew/Academic/Research/Facial_Recog_Repo/Group_50_Repo/Final_Best_Codes/Models/vit_jit_emotion_model.pt').to(device)\n",
    "model.eval()\n",
    "\n",
    "# Emotion classes\n",
    "class_names = ['angry', 'boring', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'stress', 'suprised']\n",
    "\n",
    "# Transform for face image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Haar Cascade face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Webcam input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract and preprocess the face\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        face_pil = Image.fromarray(cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB))\n",
    "        input_tensor = transform(face_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "            confidence, class_idx = torch.max(probabilities, 0)\n",
    "            emotion = class_names[class_idx.item()]\n",
    "            confidence_pct = confidence.item() * 100\n",
    "\n",
    "        # Draw bounding box and emotion label\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        label = f'{emotion} ({confidence_pct:.1f}%)'\n",
    "        cv2.putText(frame, label, (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Emotion Detection (ViT + Haar)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import timm\n",
    "from timm.data import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "from timm.optim import AdamP\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "CONFIGURATION SECTION\n",
    "- Set all hyperparameters and model selection in one place\n",
    "- Easily adjustable for experimentation\n",
    "\"\"\"\n",
    "MODEL_NAME = 'deit_small_distilled_patch16_224'  # Distilled ViT for better efficiency\n",
    "NUM_CLASSES = 9                                  # Number of emotion classes\n",
    "BATCH_SIZE = 64                                  # Larger batch size for stability\n",
    "IMG_SIZE = 224                                   # Input image size (ViT standard)\n",
    "EPOCHS = 100                                     # Maximum training epochs\n",
    "LR = 5e-5                                        # Learning rate\n",
    "WARMUP_EPOCHS = 5                                # Warmup period\n",
    "PATIENCE = 7                                     # Early stopping patience\n",
    "MIN_DELTA = 0.001                                # Minimum improvement to reset patience\n",
    "MODEL_SAVE_PATH = 'best_emotion_model.pth'       # Model save location\n",
    "\n",
    "\"\"\"\n",
    "DATA AUGMENTATION PIPELINE\n",
    "- Critical for improving model generalization\n",
    "- Techniques applied:\n",
    "  1. Random cropping (with padding)\n",
    "  2. Horizontal flipping (mirror images)\n",
    "  3. Affine transformations (rotation, scaling, translation)\n",
    "  4. Color jitter (lighting variations)\n",
    "  5. Gaussian blur (focus on structural features)\n",
    "  6. Random erasing (simulate occlusions)\n",
    "\"\"\"\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),  # Slightly larger for random crop\n",
    "    transforms.RandomCrop(IMG_SIZE),                    # Random crop to final size\n",
    "    transforms.RandomHorizontalFlip(p=0.5),             # Mirror images\n",
    "    transforms.RandomAffine(                             # Perspective/rotation variations\n",
    "        degrees=15, \n",
    "        translate=(0.1, 0.1), \n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.ColorJitter(                             # Lighting variations\n",
    "        brightness=0.2, \n",
    "        contrast=0.2, \n",
    "        saturation=0.2\n",
    "    ),\n",
    "    transforms.GaussianBlur(                             # Focus on structural features\n",
    "        kernel_size=(5, 5), \n",
    "        sigma=(0.1, 2.0)\n",
    "    ),\n",
    "    transforms.ToTensor(),                               # Convert to tensor\n",
    "    transforms.Normalize(                                # ImageNet normalization\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    transforms.RandomErasing(                            # Simulate occlusions\n",
    "        p=0.5, \n",
    "        scale=(0.02, 0.1), \n",
    "        value='random'\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),     # Fixed size\n",
    "    transforms.ToTensor(),                       # Convert to tensor\n",
    "    transforms.Normalize(                        # Same normalization\n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "DATA LOADING FUNCTION\n",
    "- Loads dataset from directory structure\n",
    "- Creates train/validation splits (85/15)\n",
    "- Applies appropriate transforms\n",
    "- Sets up data loaders with shuffling and batching\n",
    "\"\"\"\n",
    "def prepare_data(data_dir='dataset'):\n",
    "    # Load full dataset from directory structure\n",
    "    full_dataset = datasets.ImageFolder(data_dir)\n",
    "    \n",
    "    # Create train/validation splits (85/15)\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_data, val_data = random_split(\n",
    "        full_dataset, \n",
    "        [train_size, val_size]\n",
    "    )\n",
    "    \n",
    "    # Apply transforms to splits\n",
    "    train_data.dataset.transform = train_transform\n",
    "    val_data.dataset.transform = val_transform\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data, \n",
    "        batch_size=BATCH_SIZE*2,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, full_dataset.classes\n",
    "\n",
    "\"\"\"\n",
    "MODEL INITIALIZATION\n",
    "- Loads pre-trained vision transformer\n",
    "- Modifies final classification layer\n",
    "- Applies regularization techniques\n",
    "- Enables gradient checkpointing (memory optimization)\n",
    "\"\"\"\n",
    "def initialize_model(num_classes):\n",
    "    # Create model with pre-trained weights\n",
    "    model = timm.create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=num_classes,\n",
    "        distilled=False,           # Disable distillation head\n",
    "        drop_rate=0.05,             # Dropout for regularization\n",
    "        drop_path_rate=0.1          # Stochastic depth\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing (memory saving)\n",
    "    model.set_grad_checkpointing()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "TRAINING LOOP WITH EARLY STOPPING\n",
    "Core components:\n",
    "1. Mixed Precision Training - Faster computation, less memory\n",
    "2. Mixup/Cutmix Augmentation - Regularization technique\n",
    "3. Learning Rate Scheduling - Warmup + Cosine annealing\n",
    "4. Early Stopping - Halts training when validation plateaus\n",
    "\"\"\"\n",
    "def train_model(model, train_loader, val_loader, num_epochs):\n",
    "    # Setup device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function with label smoothing (prevents overconfidence)\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    \n",
    "    # Optimizer with weight decay (L2 regularization)\n",
    "    optimizer = AdamP(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    num_steps = len(train_loader) * num_epochs\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=num_steps - len(train_loader)*WARMUP_EPOCHS,\n",
    "        eta_min=LR/100\n",
    "    )\n",
    "    \n",
    "    # Warmup scheduler\n",
    "    warmup_scheduler = optim.lr_scheduler.LambdaLR(\n",
    "        optimizer,\n",
    "        lambda step: min(1.0, (step + 1) / (len(train_loader) * WARMUP_EPOCHS))\n",
    "    )\n",
    "    \n",
    "    # Mixed precision training (FP16)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=device.type=='cuda')\n",
    "    \n",
    "    # Early stopping initialization\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    \n",
    "    # Training history tracking\n",
    "    history = {'train_loss': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    # Epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # Train phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Initialize Mixup each epoch\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=0.2, \n",
    "            cutmix_alpha=1.0, \n",
    "            prob=0.5,\n",
    "            label_smoothing=0.1,\n",
    "            num_classes=NUM_CLASSES\n",
    "        )\n",
    "        \n",
    "        # Batch loop\n",
    "        for batch_idx, (inputs, targets) in enumerate(tqdm(train_loader, \n",
    "                                                          desc=f'Epoch {epoch+1}/{num_epochs}')):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Apply Mixup/Cutmix\n",
    "            inputs, targets = mixup_fn(inputs, targets)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            with torch.cuda.amp.autocast(enabled=device.type=='cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backpropagation with scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping (prevent explosions)\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Weight update\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Track loss\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Update learning rate\n",
    "            if epoch < WARMUP_EPOCHS:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Store current LR\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            history['lr'].append(current_lr)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:03d}/{num_epochs} | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"LR: {current_lr:.2e}\")\n",
    "        \n",
    "        \"\"\"\n",
    "        EARLY STOPPING MECHANISM\n",
    "        - Monitors validation accuracy improvement\n",
    "        - Saves best model checkpoint\n",
    "        - Stops training if no improvement after PATIENCE epochs\n",
    "        \"\"\"\n",
    "        # Check for improvement\n",
    "        if val_acc > best_val_acc + MIN_DELTA:\n",
    "            print(f\"Validation accuracy improved: {best_val_acc:.2f}% -> {val_acc:.2f}%\")\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "                'loss': avg_train_loss,\n",
    "            }, MODEL_SAVE_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"No improvement for {epochs_no_improve}/{PATIENCE} epochs\")\n",
    "            \n",
    "            # Check early stopping condition\n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                early_stop = True\n",
    "                print(f\"Early stopping triggered! Best accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model weights\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Training complete. Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "    return model, history\n",
    "\n",
    "\"\"\"\n",
    "VISUALIZATION FUNCTION\n",
    "- Plots training history for analysis\n",
    "- Helps identify overfitting/underfitting\n",
    "\"\"\"\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy', color='orange')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.axhline(y=max(history['val_acc']), color='r', linestyle='--', \n",
    "                label=f'Best: {max(history[\"val_acc\"]):.2f}%')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "MAIN EXECUTION FLOW\n",
    "1. Prepare data\n",
    "2. Initialize model\n",
    "3. Train with early stopping\n",
    "4. Save final model\n",
    "5. Visualize results\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    train_loader, val_loader, class_names = prepare_data()\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = initialize_model(NUM_CLASSES)\n",
    "    print(f\"Model: {MODEL_NAME} | Parameters: {sum(p.numel() for p in model.parameters())/1e6:.1f}M\")\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, history = train_model(model, train_loader, val_loader, EPOCHS)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(trained_model.state_dict(), 'final_emotion_model.pth')\n",
    "    print(\"Final model saved\")\n",
    "    \n",
    "    # Convert to TorchScript for production\n",
    "    scripted_model = torch.jit.script(trained_model)\n",
    "    scripted_model.save('emotion_model_scripted.pt')\n",
    "    print(f\"Scripted model size: {os.path.getsize('emotion_model_scripted.pt')/1e6:.1f}MB\")\n",
    "    \n",
    "    # Visualize training\n",
    "    plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.parallel import DataParallel  # For multi-GPU support\n",
    "import timm\n",
    "from timm.data import Mixup\n",
    "from timm.loss import LabelSmoothingCrossEntropy\n",
    "from timm.optim import AdamP\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ======================================================================\n",
    "# CUDA OPTIMIZATION SETUP\n",
    "# ======================================================================\n",
    "def setup_device():\n",
    "    \"\"\"Initialize CUDA devices and enable hardware optimizations\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f\"‚úÖ CUDA detected: {torch.cuda.device_count()} GPU(s) available\")\n",
    "        \n",
    "        # Enable cuDNN auto-tuner for optimal convolution algorithms\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        \n",
    "        # Enable TF32 precision for Ampere GPUs (faster with minimal precision loss)\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        \n",
    "        # Set default tensor type to CUDA for automatic device placement\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        print(\"‚ö†Ô∏è Using CPU - performance will be limited\")\n",
    "    \n",
    "    return device\n",
    "\n",
    "# Initialize device at start\n",
    "device = setup_device()\n",
    "\n",
    "# ======================================================================\n",
    "# CONFIGURATION (ADJUSTABLE PARAMETERS)\n",
    "# ======================================================================\n",
    "MODEL_NAME = 'deit_small_distilled_patch16_224'  # Distilled ViT for better efficiency\n",
    "NUM_CLASSES = 9                                  # Number of emotion classes\n",
    "BATCH_SIZE = 128 if torch.cuda.is_available() else 64  # Larger batches for GPUs\n",
    "IMG_SIZE = 224                                   # Input image size (ViT standard)\n",
    "EPOCHS = 100                                     # Maximum training epochs\n",
    "LR = 5e-5                                        # Learning rate\n",
    "WARMUP_EPOCHS = 5                                # Warmup period\n",
    "PATIENCE = 7                                     # Early stopping patience\n",
    "MIN_DELTA = 0.001                                # Minimum improvement to reset patience\n",
    "MODEL_SAVE_PATH = 'best_emotion_model.pth'       # Model save location\n",
    "\n",
    "# ======================================================================\n",
    "# DATA AUGMENTATION PIPELINE\n",
    "# ======================================================================\n",
    "def create_transforms():\n",
    "    \"\"\"Create optimized data augmentation pipelines with CUDA-aware operations\"\"\"\n",
    "    # Training augmentations (more aggressive)\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE + 32, IMG_SIZE + 32)),\n",
    "        transforms.RandomCrop(IMG_SIZE),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomAffine(\n",
    "            degrees=15, \n",
    "            translate=(0.1, 0.1), \n",
    "            scale=(0.9, 1.1)\n",
    "        ),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2, \n",
    "            contrast=0.2, \n",
    "            saturation=0.2\n",
    "        ),\n",
    "        transforms.GaussianBlur(kernel_size=(5, 5), sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), value='random'),\n",
    "    ])\n",
    "\n",
    "    # Validation transforms (minimal processing)\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "# ======================================================================\n",
    "# DATA LOADING WITH CUDA OPTIMIZATION\n",
    "# ======================================================================\n",
    "def prepare_data(data_dir='dataset'):\n",
    "    \"\"\"Load and prepare dataset with CUDA-optimized data loading\"\"\"\n",
    "    train_transform, val_transform = create_transforms()\n",
    "    \n",
    "    # Load dataset from directory structure\n",
    "    full_dataset = datasets.ImageFolder(data_dir)\n",
    "    \n",
    "    # Split dataset (85% train, 15% validation)\n",
    "    train_size = int(0.85 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_data, val_data = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    # Apply transforms\n",
    "    train_data.dataset.transform = train_transform\n",
    "    val_data.dataset.transform = val_transform\n",
    "    \n",
    "    # Configure DataLoader for optimal GPU utilization\n",
    "    num_workers = os.cpu_count()\n",
    "    train_loader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=min(8, num_workers),  # Optimize worker count\n",
    "        pin_memory=True,      # Faster data transfer to GPU\n",
    "        persistent_workers=True,  # Maintain workers between epochs\n",
    "        prefetch_factor=2     # Preload batches in background\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_data, \n",
    "        batch_size=BATCH_SIZE * 2,  # Larger batches for validation\n",
    "        num_workers=min(4, num_workers//2),\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, full_dataset.classes\n",
    "\n",
    "# ======================================================================\n",
    "# MODEL INITIALIZATION WITH PARALLELIZATION\n",
    "# ======================================================================\n",
    "def initialize_model(num_classes):\n",
    "    \"\"\"Initialize model with CUDA and multi-GPU support\"\"\"\n",
    "    # Create model with pre-trained weights\n",
    "    model = timm.create_model(\n",
    "        MODEL_NAME,\n",
    "        pretrained=True,\n",
    "        num_classes=num_classes,\n",
    "        distilled=False,\n",
    "        drop_rate=0.05,      # Regularization\n",
    "        drop_path_rate=0.1    # Stochastic depth\n",
    "    )\n",
    "    \n",
    "    # Enable gradient checkpointing to reduce memory usage\n",
    "    model.set_grad_checkpointing()\n",
    "    \n",
    "    # Move model to GPU immediately\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Enable multi-GPU training if available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"üöÄ Using {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "        model = DataParallel(model)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ======================================================================\n",
    "# CUDA-OPTIMIZED TRAINING LOOP\n",
    "# ======================================================================\n",
    "def train_model(model, train_loader, val_loader, num_epochs):\n",
    "    \"\"\"Training loop optimized for CUDA with parallel processing\"\"\"\n",
    "    # Initialize mixed precision scaler\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=device.type == 'cuda')\n",
    "    \n",
    "    # Loss function with label smoothing\n",
    "    criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "    \n",
    "    # Optimizer with weight decay\n",
    "    optimizer = AdamP(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=len(train_loader) * (num_epochs - WARMUP_EPOCHS),\n",
    "        eta_min=LR/100\n",
    "    )\n",
    "    \n",
    "    # Warmup scheduler\n",
    "    warmup_scheduler = optim.lr_scheduler.LinearLR(\n",
    "        optimizer, \n",
    "        start_factor=0.01,\n",
    "        end_factor=1.0,\n",
    "        total_iters=len(train_loader) * WARMUP_EPOCHS\n",
    "    )\n",
    "    \n",
    "    # Early stopping setup\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    early_stop = False\n",
    "    history = {'train_loss': [], 'val_acc': [], 'lr': []}\n",
    "    \n",
    "    # Epoch loop\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(f\"‚èπ Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "            \n",
    "        # ---------------------\n",
    "        # TRAINING PHASE\n",
    "        # ---------------------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Initialize Mixup each epoch\n",
    "        mixup_fn = Mixup(\n",
    "            mixup_alpha=0.2, \n",
    "            cutmix_alpha=1.0, \n",
    "            prob=0.5,\n",
    "            label_smoothing=0.1,\n",
    "            num_classes=NUM_CLASSES\n",
    "        )\n",
    "        \n",
    "        # Batch processing with CUDA optimizations\n",
    "        for inputs, targets in tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{num_epochs}'):\n",
    "            # Asynchronous data transfer to GPU (non-blocking)\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            \n",
    "            # Apply Mixup/Cutmix\n",
    "            inputs, targets = mixup_fn(inputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision context (FP16 operations)\n",
    "            with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backpropagation with gradient scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient clipping to prevent explosions\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Optimizer step with scaling\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Update learning rate\n",
    "            if epoch < WARMUP_EPOCHS:\n",
    "                warmup_scheduler.step()\n",
    "            else:\n",
    "                scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # ---------------------\n",
    "        # VALIDATION PHASE\n",
    "        # ---------------------\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(val_loader, desc='Validating'):\n",
    "                inputs = inputs.to(device, non_blocking=True)\n",
    "                targets = targets.to(device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision validation (faster)\n",
    "                with torch.cuda.amp.autocast(enabled=device.type == 'cuda'):\n",
    "                    outputs = model(inputs)\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_loss = train_loss / len(train_loader)\n",
    "        val_acc = 100 * correct / total\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(avg_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:03d}/{num_epochs} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # ---------------------\n",
    "        # EARLY STOPPING LOGIC\n",
    "        # ---------------------\n",
    "        if val_acc > best_val_acc + MIN_DELTA:\n",
    "            print(f\"üìà Accuracy improved: {best_val_acc:.2f}% ‚Üí {val_acc:.2f}%\")\n",
    "            best_val_acc = val_acc\n",
    "            epochs_no_improve = 0\n",
    "            \n",
    "            # Save best model (handle parallel models)\n",
    "            model_state = model.module.state_dict() if isinstance(model, DataParallel) else model.state_dict()\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model_state,\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': val_acc,\n",
    "            }, MODEL_SAVE_PATH)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"‚è≥ No improvement: {epochs_no_improve}/{PATIENCE}\")\n",
    "            \n",
    "            if epochs_no_improve >= PATIENCE:\n",
    "                early_stop = True\n",
    "                print(f\"üõë Early stopping! Best accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    # Load best model for return\n",
    "    checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
    "    if isinstance(model, DataParallel):\n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"üèÅ Training complete. Best accuracy: {best_val_acc:.2f}%\")\n",
    "    return model, history\n",
    "\n",
    "# ======================================================================\n",
    "# VISUALIZATION\n",
    "# ======================================================================\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Visualize training progress and metrics\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy', color='orange')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.axhline(y=max(history['val_acc']), color='r', linestyle='--', \n",
    "                label=f'Best: {max(history[\"val_acc\"]):.2f}%')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "# ======================================================================\n",
    "# MAIN EXECUTION FLOW\n",
    "# ======================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting emotion classification training\")\n",
    "    print(f\"üîß Configuration: {MODEL_NAME} | Batch: {BATCH_SIZE} | LR: {LR}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    train_loader, val_loader, class_names = prepare_data()\n",
    "    print(f\"üìä Classes: {class_names}\")\n",
    "    print(f\"üì¶ Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Initialize model\n",
    "    model = initialize_model(NUM_CLASSES)\n",
    "    param_count = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"üß† Model: {MODEL_NAME} | Parameters: {param_count/1e6:.1f}M\")\n",
    "    \n",
    "    # Train model\n",
    "    trained_model, history = train_model(model, train_loader, val_loader, EPOCHS)\n",
    "    \n",
    "    # Save final model\n",
    "    final_state = trained_model.module.state_dict() if isinstance(trained_model, DataParallel) else trained_model.state_dict()\n",
    "    torch.save(final_state, 'emotion_model_final.pth')\n",
    "    \n",
    "    # Save for inference\n",
    "    trained_model.eval()\n",
    "    example = torch.rand(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "    \n",
    "    # Optimized model export\n",
    "    if device.type == 'cuda':\n",
    "        with torch.cuda.amp.autocast():\n",
    "            traced_model = torch.jit.trace(trained_model, example)\n",
    "    else:\n",
    "        traced_model = torch.jit.trace(trained_model, example)\n",
    "        \n",
    "    traced_model.save('emotion_model_traced.pt')\n",
    "    model_size = os.path.getsize('emotion_model_traced.pt') / 1e6\n",
    "    print(f\"üíæ Model saved: {model_size:.1f}MB\")\n",
    "    \n",
    "    # Visualize training\n",
    "    plot_training_history(history)\n",
    "    print(\"‚úÖ Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
