{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2341dc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 14:12:16.432176: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-22 14:12:16.564216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747903336.620861  457391 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747903336.640112  457391 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747903336.743713  457391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747903336.743742  457391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747903336.743743  457391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747903336.743744  457391 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 14:12:16.755615: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d84fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/mnt/d/Group50/Facial_Emotion_Recongnition/Datasets/VideoDataset2\"\n",
    "IMG_SIZE = (64, 64)\n",
    "SEQUENCE_LENGTH = 20\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8e0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted(os.listdir(DATASET_PATH))\n",
    "label_map = {label: idx for idx, label in enumerate(class_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06be4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_path, max_frames=SEQUENCE_LENGTH):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, IMG_SIZE)\n",
    "        frame = frame.astype(np.float32) / 255.0\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(np.zeros((IMG_SIZE[1], IMG_SIZE[0], 3)))\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading video data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing neutral: 100%|██████████| 1714/1714 [00:39<00:00, 43.50it/s]\n",
      "Processing sadness: 100%|██████████| 1714/1714 [00:26<00:00, 64.68it/s]\n",
      "Processing surprise: 100%|██████████| 1714/1714 [00:15<00:00, 109.96it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "print(\"\\U0001F4E5 Loading video data...\")\n",
    "for label in class_labels:\n",
    "    class_dir = os.path.join(DATASET_PATH, label)\n",
    "    for file in tqdm(os.listdir(class_dir), desc=f\"Processing {label}\"):\n",
    "        if file.endswith('.mp4'):\n",
    "            video_path = os.path.join(class_dir, file)\n",
    "            frames = video_to_frames(video_path)\n",
    "            X.append(frames)\n",
    "            y.append(label_map[label])\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(np.array(y), num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b63948",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493ffd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/group50/anaconda3/envs/condaenv/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1747902498.109603  228352 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,227,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m) │           \u001b[38;5;34m896\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_1              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_4              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m12544\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m3,227,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,251,586</span> (12.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,251,586\u001b[0m (12.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,251,586</span> (12.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,251,586\u001b[0m (12.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=(SEQUENCE_LENGTH, IMG_SIZE[0], IMG_SIZE[1], 3)),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    TimeDistributed(Conv2D(64, (3, 3), activation='relu')),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(64),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b2958",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m history = \u001b[43mmodel\u001b[49m.fit(\n\u001b[32m      4\u001b[39m     X_train, y_train,\n\u001b[32m      5\u001b[39m     validation_data=(X_val, y_val),\n\u001b[32m      6\u001b[39m     epochs=EPOCHS,\n\u001b[32m      7\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef588638",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model_layout.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f168e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('emotion_cnn_lstm_model.h5')\n",
    "print(\"✅ Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beef881",
   "metadata": {},
   "source": [
    "New Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e47314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing neutral: 100%|██████████| 1714/1714 [01:07<00:00, 25.56it/s]\n",
      "Processing sadness: 100%|██████████| 1714/1714 [00:53<00:00, 31.97it/s]\n",
      "Processing surprise: 100%|██████████| 1714/1714 [00:41<00:00, 41.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_PATH = \"/mnt/d/Group50/Facial_Emotion_Recongnition/Datasets/VideoDataset2\"\n",
    "OUTPUT_PATH = \"processed_videos\"\n",
    "MAX_FRAMES = 30\n",
    "IMG_SIZE = (64, 64)\n",
    "\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "metadata = []\n",
    "\n",
    "def video_to_frames(video_path, max_frames=MAX_FRAMES):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while len(frames) < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, IMG_SIZE)\n",
    "        frame = frame / 255.0  # Normalize\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    \n",
    "    # Pad if fewer than max_frames\n",
    "    while len(frames) < max_frames:\n",
    "        frames.append(np.zeros((IMG_SIZE[1], IMG_SIZE[0], 3)))\n",
    "    \n",
    "    return np.array(frames)\n",
    "\n",
    "# Save .npy files and metadata\n",
    "for label in os.listdir(DATASET_PATH):\n",
    "    class_dir = os.path.join(DATASET_PATH, label)\n",
    "    for file in tqdm(os.listdir(class_dir), desc=f\"Processing {label}\"):\n",
    "        if file.endswith(\".mp4\"):\n",
    "            video_path = os.path.join(class_dir, file)\n",
    "            frames = video_to_frames(video_path)\n",
    "            video_id = os.path.splitext(file)[0]\n",
    "            out_file = os.path.join(OUTPUT_PATH, f\"{video_id}.npy\")\n",
    "            np.save(out_file, frames)\n",
    "            metadata.append({\"video_id\": video_id, \"label\": label})\n",
    "\n",
    "# Save metadata\n",
    "with open(os.path.join(OUTPUT_PATH, \"metadata.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3000c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load metadata\n",
    "with open(os.path.join(OUTPUT_PATH, \"metadata.json\"), \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Encode labels\n",
    "labels = [item[\"label\"] for item in metadata]\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Add encoded labels to metadata\n",
    "for i, item in enumerate(metadata):\n",
    "    item[\"label_encoded\"] = int(y_encoded[i])  # Convert to Python int\n",
    "\n",
    "\n",
    "# Save updated metadata\n",
    "with open(os.path.join(OUTPUT_PATH, \"metadata_encoded.json\"), \"w\") as f:\n",
    "    json.dump(metadata, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cfb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class VideoDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, metadata, batch_size, num_classes, data_path, shuffle=True):\n",
    "        self.metadata = metadata\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.data_path = data_path\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.metadata) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_metadata = self.metadata[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        X, y = self.__data_generation(batch_metadata)\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.metadata)\n",
    "\n",
    "    def __data_generation(self, batch_metadata):\n",
    "        X = []\n",
    "        y = []\n",
    "        for item in batch_metadata:\n",
    "            file_path = os.path.join(self.data_path, f\"{item['video_id']}.npy\")\n",
    "            frames = np.load(file_path)\n",
    "            X.append(frames)\n",
    "            y.append(item[\"label_encoded\"])\n",
    "        X = np.array(X)\n",
    "        y = to_categorical(y, num_classes=self.num_classes)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5089e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/group50/anaconda3/envs/condaenv/lib/python3.11/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "I0000 00:00:1747904636.631589  542302 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/group50/anaconda3/envs/condaenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747904641.166638  626201 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 110ms/step - accuracy: 0.4034 - loss: 1.0867 - val_accuracy: 0.4985 - val_loss: 0.9986\n",
      "Epoch 2/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 103ms/step - accuracy: 0.6336 - loss: 0.7914 - val_accuracy: 0.7211 - val_loss: 0.6286\n",
      "Epoch 3/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 107ms/step - accuracy: 0.8445 - loss: 0.4132 - val_accuracy: 0.7804 - val_loss: 0.5192\n",
      "Epoch 4/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 101ms/step - accuracy: 0.9338 - loss: 0.2055 - val_accuracy: 0.8251 - val_loss: 0.4599\n",
      "Epoch 5/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 108ms/step - accuracy: 0.9739 - loss: 0.0962 - val_accuracy: 0.8144 - val_loss: 0.5394\n",
      "Epoch 6/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 109ms/step - accuracy: 0.9799 - loss: 0.0648 - val_accuracy: 0.7911 - val_loss: 0.6656\n",
      "Epoch 7/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 111ms/step - accuracy: 0.9897 - loss: 0.0406 - val_accuracy: 0.8445 - val_loss: 0.5846\n",
      "Epoch 8/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 109ms/step - accuracy: 0.9893 - loss: 0.0347 - val_accuracy: 0.8280 - val_loss: 0.5824\n",
      "Epoch 9/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 107ms/step - accuracy: 0.9936 - loss: 0.0206 - val_accuracy: 0.8367 - val_loss: 0.6714\n",
      "Epoch 10/10\n",
      "\u001b[1m515/515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 111ms/step - accuracy: 0.9867 - loss: 0.0385 - val_accuracy: 0.8260 - val_loss: 0.7280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fd8d6067250>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load encoded metadata\n",
    "with open(os.path.join(OUTPUT_PATH, \"metadata_encoded.json\"), \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Split into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_meta, val_meta = train_test_split(metadata, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generators\n",
    "train_gen = VideoDataGenerator(train_meta, batch_size=8, num_classes=num_classes, data_path=OUTPUT_PATH)\n",
    "val_gen = VideoDataGenerator(val_meta, batch_size=8, num_classes=num_classes, data_path=OUTPUT_PATH)\n",
    "\n",
    "# Build model (example)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    TimeDistributed(Conv2D(16, (3, 3), activation='relu'), input_shape=(MAX_FRAMES, IMG_SIZE[1], IMG_SIZE[0], 3)),\n",
    "    TimeDistributed(MaxPooling2D((2, 2))),\n",
    "    TimeDistributed(Flatten()),\n",
    "    LSTM(64),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "620ffa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved!\n"
     ]
    }
   ],
   "source": [
    "model.save('emotion_cnn_lstm_model.h5')\n",
    "print(\"✅ Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d482338",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      3\u001b[39m plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m plt.plot(\u001b[43mhistory\u001b[49m.model[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mTrain Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m plt.plot(history.model[\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mVal Accuracy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m plt.legend()\n",
      "\u001b[31mNameError\u001b[39m: name 'history' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAGyCAYAAADau9wtAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAG+NJREFUeJzt3W1sleX9wPFfKfZUM1vZGOVhVaabcz6BgnT1Icals4mGjReLnS7AiA/TMaM02wRR6iNl/pWQKEpEnXsxB5tRYwapc92IUVmIQBOdqFF0MGMrbLNldWulvf8vjN0qoJzah6vl80nOi15c97mvc6X67X16Tk9BlmVZAABDbtRQLwAA+JAoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCLyjvIzzzwTM2fOjIkTJ0ZBQUE88cQTn3rMhg0b4vTTT49cLhdf+cpX4uGHH+7DUgFgZMs7yu3t7TFlypRYuXLlQc1/880348ILL4zzzjsvmpqa4tprr43LLrssnnrqqbwXCwAjWcFn+UCKgoKCePzxx2PWrFkHnHPdddfFunXr4qWXXuoZ+973vhfvvfdeNDQ09PXUADDijB7oE2zcuDGqqqp6jVVXV8e11157wGM6Ojqio6Oj5+vu7u74xz/+EV/4wheioKBgoJYKAAcly7LYs2dPTJw4MUaN6r+XZw14lJubm6OsrKzXWFlZWbS1tcW///3vOPzww/c5pr6+Pm6++eaBXhoAfCY7d+6ML33pS/12fwMe5b5YtGhR1NbW9nzd2toaRx99dOzcuTNKSkqGcGUAENHW1hbl5eVx5JFH9uv9DniUx48fHy0tLb3GWlpaoqSkZL9XyRERuVwucrncPuMlJSWiDEAy+vtXqgP+PuXKyspobGzsNfb0009HZWXlQJ8aAIaVvKP8r3/9K5qamqKpqSkiPnzLU1NTU+zYsSMiPnzqec6cOT3zr7zyyti+fXv87Gc/i1deeSXuvffe+M1vfhMLFizon0cAACNE3lF+4YUX4rTTTovTTjstIiJqa2vjtNNOiyVLlkRExDvvvNMT6IiIL3/5y7Fu3bp4+umnY8qUKXHXXXfFAw88ENXV1f30EABgZPhM71MeLG1tbVFaWhqtra1+pwzAkBuoLvnb1wCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBE9CnKK1eujMmTJ0dxcXFUVFTEpk2bPnH+ihUr4mtf+1ocfvjhUV5eHgsWLIj//Oc/fVowAIxUeUd57dq1UVtbG3V1dbFly5aYMmVKVFdXx7vvvrvf+Y888kgsXLgw6urqYtu2bfHggw/G2rVr4/rrr//MiweAkSTvKC9fvjwuv/zymDdvXpx44omxatWqOOKII+Khhx7a7/znn38+zjrrrLjkkkti8uTJcf7558fFF1/8qVfXAHCoySvKnZ2dsXnz5qiqqvrvHYwaFVVVVbFx48b9HnPmmWfG5s2beyK8ffv2WL9+fVxwwQUHPE9HR0e0tbX1ugHASDc6n8m7d++Orq6uKCsr6zVeVlYWr7zyyn6PueSSS2L37t1x9tlnR5ZlsXfv3rjyyis/8enr+vr6uPnmm/NZGgAMewP+6usNGzbE0qVL4957740tW7bEY489FuvWrYtbb731gMcsWrQoWltbe247d+4c6GUCwJDL60p57NixUVhYGC0tLb3GW1paYvz48fs95sYbb4zZs2fHZZddFhERp5xySrS3t8cVV1wRixcvjlGj9v25IJfLRS6Xy2dpADDs5XWlXFRUFNOmTYvGxsaese7u7mhsbIzKysr9HvP+++/vE97CwsKIiMiyLN/1AsCIldeVckREbW1tzJ07N6ZPnx4zZsyIFStWRHt7e8ybNy8iIubMmROTJk2K+vr6iIiYOXNmLF++PE477bSoqKiI119/PW688caYOXNmT5wBgD5EuaamJnbt2hVLliyJ5ubmmDp1ajQ0NPS8+GvHjh29roxvuOGGKCgoiBtuuCHefvvt+OIXvxgzZ86M22+/vf8eBQCMAAXZMHgOua2tLUpLS6O1tTVKSkqGejkAHOIGqkv+9jUAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEX2K8sqVK2Py5MlRXFwcFRUVsWnTpk+c/95778X8+fNjwoQJkcvl4vjjj4/169f3acEAMFKNzveAtWvXRm1tbaxatSoqKipixYoVUV1dHa+++mqMGzdun/mdnZ3xrW99K8aNGxePPvpoTJo0Kf7617/GUUcd1R/rB4ARoyDLsiyfAyoqKuKMM86Ie+65JyIiuru7o7y8PK6++upYuHDhPvNXrVoV//d//xevvPJKHHbYYX1aZFtbW5SWlkZra2uUlJT06T4AoL8MVJfyevq6s7MzNm/eHFVVVf+9g1GjoqqqKjZu3LjfY5588smorKyM+fPnR1lZWZx88smxdOnS6OrqOuB5Ojo6oq2trdcNAEa6vKK8e/fu6OrqirKysl7jZWVl0dzcvN9jtm/fHo8++mh0dXXF+vXr48Ybb4y77rorbrvttgOep76+PkpLS3tu5eXl+SwTAIalAX/1dXd3d4wbNy7uv//+mDZtWtTU1MTixYtj1apVBzxm0aJF0dra2nPbuXPnQC8TAIZcXi/0Gjt2bBQWFkZLS0uv8ZaWlhg/fvx+j5kwYUIcdthhUVhY2DP29a9/PZqbm6OzszOKior2OSaXy0Uul8tnaQAw7OV1pVxUVBTTpk2LxsbGnrHu7u5obGyMysrK/R5z1llnxeuvvx7d3d09Y6+99lpMmDBhv0EGgENV3k9f19bWxurVq+OXv/xlbNu2La666qpob2+PefPmRUTEnDlzYtGiRT3zr7rqqvjHP/4R11xzTbz22muxbt26WLp0acyfP7//HgUAjAB5v0+5pqYmdu3aFUuWLInm5uaYOnVqNDQ09Lz4a8eOHTFq1H9bX15eHk899VQsWLAgTj311Jg0aVJcc801cd111/XfowCAESDv9ykPBe9TBiAlSbxPGQAYOKIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJ6FOUV65cGZMnT47i4uKoqKiITZs2HdRxa9asiYKCgpg1a1ZfTgsAI1reUV67dm3U1tZGXV1dbNmyJaZMmRLV1dXx7rvvfuJxb731VvzkJz+Jc845p8+LBYCRLO8oL1++PC6//PKYN29enHjiibFq1ao44ogj4qGHHjrgMV1dXfH9738/br755jj22GM/04IBYKTKK8qdnZ2xefPmqKqq+u8djBoVVVVVsXHjxgMed8stt8S4cePi0ksvPajzdHR0RFtbW68bAIx0eUV59+7d0dXVFWVlZb3Gy8rKorm5eb/HPPvss/Hggw/G6tWrD/o89fX1UVpa2nMrLy/PZ5kAMCwN6Kuv9+zZE7Nnz47Vq1fH2LFjD/q4RYsWRWtra89t586dA7hKAEjD6Hwmjx07NgoLC6OlpaXXeEtLS4wfP36f+W+88Ua89dZbMXPmzJ6x7u7uD088enS8+uqrcdxxx+1zXC6Xi1wul8/SAGDYy+tKuaioKKZNmxaNjY09Y93d3dHY2BiVlZX7zD/hhBPixRdfjKampp7bt7/97TjvvPOiqanJ09IA8D/yulKOiKitrY25c+fG9OnTY8aMGbFixYpob2+PefPmRUTEnDlzYtKkSVFfXx/FxcVx8skn9zr+qKOOiojYZxwADnV5R7mmpiZ27doVS5Ysiebm5pg6dWo0NDT0vPhrx44dMWqUPxQGAPkqyLIsG+pFfJq2trYoLS2N1tbWKCkpGerlAHCIG6guuaQFgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJKJPUV65cmVMnjw5iouLo6KiIjZt2nTAuatXr45zzjknxowZE2PGjImqqqpPnA8Ah6q8o7x27dqora2Nurq62LJlS0yZMiWqq6vj3Xff3e/8DRs2xMUXXxx/+tOfYuPGjVFeXh7nn39+vP3225958QAwkhRkWZblc0BFRUWcccYZcc8990RERHd3d5SXl8fVV18dCxcu/NTju7q6YsyYMXHPPffEnDlzDuqcbW1tUVpaGq2trVFSUpLPcgGg3w1Ul/K6Uu7s7IzNmzdHVVXVf+9g1KioqqqKjRs3HtR9vP/++/HBBx/E5z//+QPO6ejoiLa2tl43ABjp8ory7t27o6urK8rKynqNl5WVRXNz80Hdx3XXXRcTJ07sFfaPq6+vj9LS0p5beXl5PssEgGFpUF99vWzZslizZk08/vjjUVxcfMB5ixYtitbW1p7bzp07B3GVADA0RuczeezYsVFYWBgtLS29xltaWmL8+PGfeOydd94Zy5Ytiz/84Q9x6qmnfuLcXC4XuVwun6UBwLCX15VyUVFRTJs2LRobG3vGuru7o7GxMSorKw943B133BG33nprNDQ0xPTp0/u+WgAYwfK6Uo6IqK2tjblz58b06dNjxowZsWLFimhvb4958+ZFRMScOXNi0qRJUV9fHxERP//5z2PJkiXxyCOPxOTJk3t+9/y5z30uPve5z/XjQwGA4S3vKNfU1MSuXbtiyZIl0dzcHFOnTo2GhoaeF3/t2LEjRo367wX4fffdF52dnfHd73631/3U1dXFTTfd9NlWDwAjSN7vUx4K3qcMQEqSeJ8yADBwRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASIQoA0AiRBkAEiHKAJAIUQaARIgyACRClAEgEaIMAIkQZQBIhCgDQCJEGQASIcoAkAhRBoBEiDIAJEKUASARogwAiRBlAEiEKANAIkQZABLRpyivXLkyJk+eHMXFxVFRURGbNm36xPm//e1v44QTToji4uI45ZRTYv369X1aLACMZHlHee3atVFbWxt1dXWxZcuWmDJlSlRXV8e777673/nPP/98XHzxxXHppZfG1q1bY9asWTFr1qx46aWXPvPiAWAkKciyLMvngIqKijjjjDPinnvuiYiI7u7uKC8vj6uvvjoWLly4z/yamppob2+P3/3udz1j3/jGN2Lq1KmxatWqgzpnW1tblJaWRmtra5SUlOSzXADodwPVpdH5TO7s7IzNmzfHokWLesZGjRoVVVVVsXHjxv0es3Hjxqitre01Vl1dHU888cQBz9PR0REdHR09X7e2tkbEh5sAAEPtox7leV37qfKK8u7du6OrqyvKysp6jZeVlcUrr7yy32Oam5v3O7+5ufmA56mvr4+bb755n/Hy8vJ8lgsAA+rvf/97lJaW9tv95RXlwbJo0aJeV9fvvfdeHHPMMbFjx45+ffCHqra2tigvL4+dO3f6dUA/saf9y372P3vav1pbW+Poo4+Oz3/+8/16v3lFeezYsVFYWBgtLS29xltaWmL8+PH7PWb8+PF5zY+IyOVykcvl9hkvLS31zdSPSkpK7Gc/s6f9y372P3vav0aN6t93Fud1b0VFRTFt2rRobGzsGevu7o7GxsaorKzc7zGVlZW95kdEPP300wecDwCHqryfvq6trY25c+fG9OnTY8aMGbFixYpob2+PefPmRUTEnDlzYtKkSVFfXx8REddcc02ce+65cdddd8WFF14Ya9asiRdeeCHuv//+/n0kADDM5R3lmpqa2LVrVyxZsiSam5tj6tSp0dDQ0PNirh07dvS6nD/zzDPjkUceiRtuuCGuv/76+OpXvxpPPPFEnHzyyQd9zlwuF3V1dft9Spv82c/+Z0/7l/3sf/a0fw3Ufub9PmUAYGD429cAkAhRBoBEiDIAJEKUASARyUTZx0H2r3z2c/Xq1XHOOefEmDFjYsyYMVFVVfWp+38oyvd79CNr1qyJgoKCmDVr1sAucJjJdz/fe++9mD9/fkyYMCFyuVwcf/zx/rv/mHz3dMWKFfG1r30tDj/88CgvL48FCxbEf/7zn0FabdqeeeaZmDlzZkycODEKCgo+8fMaPrJhw4Y4/fTTI5fLxVe+8pV4+OGH8z9xloA1a9ZkRUVF2UMPPZT95S9/yS6//PLsqKOOylpaWvY7/7nnnssKCwuzO+64I3v55ZezG264ITvssMOyF198cZBXnqZ89/OSSy7JVq5cmW3dujXbtm1b9oMf/CArLS3N/va3vw3yytOV755+5M0338wmTZqUnXPOOdl3vvOdwVnsMJDvfnZ0dGTTp0/PLrjgguzZZ5/N3nzzzWzDhg1ZU1PTIK88Xfnu6a9+9assl8tlv/rVr7I333wze+qpp7IJEyZkCxYsGOSVp2n9+vXZ4sWLs8ceeyyLiOzxxx//xPnbt2/PjjjiiKy2tjZ7+eWXs7vvvjsrLCzMGhoa8jpvElGeMWNGNn/+/J6vu7q6sokTJ2b19fX7nX/RRRdlF154Ya+xioqK7Ic//OGArnO4yHc/P27v3r3ZkUcemf3yl78cqCUOO33Z071792Znnnlm9sADD2Rz584V5f+R737ed9992bHHHpt1dnYO1hKHnXz3dP78+dk3v/nNXmO1tbXZWWedNaDrHI4OJso/+9nPspNOOqnXWE1NTVZdXZ3XuYb86euPPg6yqqqqZ+xgPg7yf+dHfPhxkAeafyjpy35+3Pvvvx8ffPBBv/+h9eGqr3t6yy23xLhx4+LSSy8djGUOG33ZzyeffDIqKytj/vz5UVZWFieffHIsXbo0urq6BmvZSevLnp555pmxefPmnqe4t2/fHuvXr48LLrhgUNY80vRXl4b8U6IG6+MgDxV92c+Pu+6662LixIn7fIMdqvqyp88++2w8+OCD0dTUNAgrHF76sp/bt2+PP/7xj/H9738/1q9fH6+//nr86Ec/ig8++CDq6uoGY9lJ68ueXnLJJbF79+44++yzI8uy2Lt3b1x55ZVx/fXXD8aSR5wDdamtrS3+/e9/x+GHH35Q9zPkV8qkZdmyZbFmzZp4/PHHo7i4eKiXMyzt2bMnZs+eHatXr46xY8cO9XJGhO7u7hg3blzcf//9MW3atKipqYnFixfHqlWrhnppw9aGDRti6dKlce+998aWLVvisccei3Xr1sWtt9461Es7pA35lfJgfRzkoaIv+/mRO++8M5YtWxZ/+MMf4tRTTx3IZQ4r+e7pG2+8EW+99VbMnDmzZ6y7uzsiIkaPHh2vvvpqHHfccQO76IT15Xt0woQJcdhhh0VhYWHP2Ne//vVobm6Ozs7OKCoqGtA1p64ve3rjjTfG7Nmz47LLLouIiFNOOSXa29vjiiuuiMWLF/f7RxKOdAfqUklJyUFfJUckcKXs4yD7V1/2MyLijjvuiFtvvTUaGhpi+vTpg7HUYSPfPT3hhBPixRdfjKampp7bt7/97TjvvPOiqakpysvLB3P5yenL9+hZZ50Vr7/+es8PNxERr732WkyYMOGQD3JE3/b0/fff3ye8H/3Qk/lIhLz1W5fyew3awFizZk2Wy+Wyhx9+OHv55ZezK664IjvqqKOy5ubmLMuybPbs2dnChQt75j/33HPZ6NGjszvvvDPbtm1bVldX5y1R/yPf/Vy2bFlWVFSUPfroo9k777zTc9uzZ89QPYTk5LunH+fV173lu587duzIjjzyyOzHP/5x9uqrr2a/+93vsnHjxmW33XbbUD2E5OS7p3V1ddmRRx6Z/frXv862b9+e/f73v8+OO+647KKLLhqqh5CUPXv2ZFu3bs22bt2aRUS2fPnybOvWrdlf//rXLMuybOHChdns2bN75n/0lqif/vSn2bZt27KVK1cO37dEZVmW3X333dnRRx+dFRUVZTNmzMj+/Oc/9/zbueeem82dO7fX/N/85jfZ8ccfnxUVFWUnnXRStm7dukFecdry2c9jjjkmi4h9bnV1dYO/8ITl+z36v0R5X/nu5/PPP59VVFRkuVwuO/bYY7Pbb78927t37yCvOm357OkHH3yQ3XTTTdlxxx2XFRcXZ+Xl5dmPfvSj7J///OfgLzxBf/rTn/b7/8WP9nDu3LnZueeeu88xU6dOzYqKirJjjz02+8UvfpH3eX10IwAkYsh/pwwAfEiUASARogwAiRBlAEiEKANAIkQZABIhygCQCFEGgESIMgAkQpQBIBGiDACJEGUASMT/AzV8KXCtNCKqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.model['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.model['val_accuracy'], label='Val Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176ec24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "model = load_model('emotion_cnn_lstm_model.h5')\n",
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, IMG_SIZE)\n",
    "        face = face.astype(np.float32) / 255.0\n",
    "        sequence.append(face)\n",
    "        if len(sequence) == SEQUENCE_LENGTH:\n",
    "            input_seq = np.expand_dims(sequence, axis=0)\n",
    "            preds = model.predict(input_seq)\n",
    "            emotion = class_labels[np.argmax(preds)]\n",
    "            cv2.putText(frame, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "            sequence = []\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    cv2.imshow('Real-time Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
