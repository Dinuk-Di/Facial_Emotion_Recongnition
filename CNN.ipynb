{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d957b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c22c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__() # Super is indeed super\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=(5,5),bias=True)\n",
    "        # for single grayscale image use 6 different kernls of size (5,5) to produce 6 diff feature maps \n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size=(3,3))\n",
    "        # from the existing 6 already feat maps, use 12 different kernal filters of size (3,3) to get TOTAL of 12 new feat\n",
    "        \n",
    "        self.dense_1 = nn.Linear(in_features=12*4*4,out_features=128) # WHYY (12*4*4) it is explained in the end\n",
    "        # Flatten the output of conv_2d in 12*4*4\n",
    "        \n",
    "        self.fc_2 = nn.Linear(in_features=128,out_features=64)\n",
    "        # Fully Connected = fc_2 = Dense Layer = dense_2\n",
    "        \n",
    "        self.out = nn.Linear(in_features=64,out_features=9)\n",
    "        # output layer. Output number of neurons = num of classes for classification & 1 for regression\n",
    "        \n",
    "    \n",
    "    def forward(self,t):\n",
    "        '''\n",
    "        implement a forward pass on a Tensor 't' of rank 'R'\n",
    "        '''\n",
    "        # input  layer 1 though it is never needed\n",
    "        # t = t\n",
    "        \n",
    "        # second layer. Layer is a mix of functions that has weights \n",
    "        t = self.conv_1(t) # works by calling __call__() method inside class\n",
    "        t = F.relu(input=t,) # it is not a layer but Function (layers have weights, activations don't)\n",
    "        t = F.max_pool2d(t,kernel_size=(3,3),stride=2) # max pooling\n",
    "        \n",
    "        # third layer\n",
    "        t = self.conv_2(t) # works by calling __call__() method inside class\n",
    "        t = F.relu(input=t,) # it is not a leyer but Function as layers have weights\n",
    "        t = F.max_pool2d(t,kernel_size=(3,3),stride=2) # max pool\n",
    "        \n",
    "        # fourth layer\n",
    "        t = t.reshape(-1,12*4*4) \n",
    "        # due to Conv and pooling operations, our image has been reduced from (1,28,28) to (4,4)\n",
    "        # use ((input_size - filter_size + 2*padding)/stride )+1  for each  cov and max_pool \n",
    "        # it assumes input and kernel size are square\n",
    "        t = self.dense_1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # Fifth layer\n",
    "        t = self.fc_2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # output\n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t,dim=1)\n",
    "        # commented because loss function used will be cross_entropy which has softmax behind the scenes\n",
    "        return t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
